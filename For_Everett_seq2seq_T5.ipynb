{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8daac09e-3464-4c70-b827-73d9c1cefe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmoradi/Downloads/colab_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-26 21:56:28.064620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-26 21:56:28.076275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-26 21:56:28.079778: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-26 21:56:28.089480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-26 21:56:28.786319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True\n",
      "CUDA_VISIBLE_DEVICES: 0,1\n",
      "Using device: cuda\n",
      "Initial Data Sample:\n",
      "    Dataset                                              Model  \\\n",
      "0  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "1  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "2  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "3  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "4  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "\n",
      "                                  True Transcription  \\\n",
      "0  lufthansa four three nine three descend to fli...   \n",
      "1  lufthansa four three nine three descend to fli...   \n",
      "2  lufthansa four three nine three descend to fli...   \n",
      "3  lufthansa four three nine three descend to fli...   \n",
      "4  lufthansa four three nine three descend to fli...   \n",
      "\n",
      "                             Predicted Transcription  WER (word Error rate)  \\\n",
      "0  loflans are fourd three nine three-decent flig...               0.583333   \n",
      "1  lufthansa four three nine three descend flight...               0.083333   \n",
      "2  lufthansa four three nine three descend to fli...               0.000000   \n",
      "3  lufthansa four three nine three descend to fli...               0.000000   \n",
      "4  loove danz a fourthreenine threeticent fuliht ...               0.750000   \n",
      "\n",
      "   CER (character Error Rate)  Cosin ->  Blue ->  \\\n",
      "0                    0.200000       0.3      0.1   \n",
      "1                    0.042857       NaN      NaN   \n",
      "2                    0.000000       NaN      NaN   \n",
      "3                    0.000000       NaN      NaN   \n",
      "4                    0.285714       NaN      NaN   \n",
      "\n",
      "   Papers we used their dataset - measure of similarity  \\\n",
      "0                                                0.4      \n",
      "1                                                NaN      \n",
      "2                                                NaN      \n",
      "3                                                NaN      \n",
      "4                                                NaN      \n",
      "\n",
      "  Correct (CER threashold 0.2<=)  Accuracy  F1-Score  Precision  Recall  \\\n",
      "0                              1         1         0          0       0   \n",
      "1                              1         0         0          0       0   \n",
      "2                              1         1         1          1       1   \n",
      "3                              1         1         1          1       1   \n",
      "4                              0         0         0          0       0   \n",
      "\n",
      "   Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17  \n",
      "0          NaN         NaN     Model A     Model B  \n",
      "1          NaN  Dataset-1          NaN         NaN  \n",
      "2          NaN    Avg. WER         0.2        0.19  \n",
      "3          NaN    Avg. CER         0.1        0.09  \n",
      "4          NaN  Avg. Cosin        0.15         NaN  \n",
      "Dataset size before cleaning: (23615, 18)\n",
      "Dataset size after cleaning: (23596, 18)\n",
      "Processed Data Sample:\n",
      "    Dataset                                              Model  \\\n",
      "0  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "1  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "2  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "3  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "4  atcosim  c:\\Users\\tim3l\\OneDrive\\Desktop\\Local_Wav2Vec\\...   \n",
      "\n",
      "                                  True Transcription  \\\n",
      "0  lufthansa four three nine three descend to fli...   \n",
      "1  lufthansa four three nine three descend to fli...   \n",
      "2  lufthansa four three nine three descend to fli...   \n",
      "3  lufthansa four three nine three descend to fli...   \n",
      "4  lufthansa four three nine three descend to fli...   \n",
      "\n",
      "                             Predicted Transcription  WER (word Error rate)  \\\n",
      "0  loflans are fourd three nine three-decent flig...               0.583333   \n",
      "1  lufthansa four three nine three descend flight...               0.083333   \n",
      "2  lufthansa four three nine three descend to fli...               0.000000   \n",
      "3  lufthansa four three nine three descend to fli...               0.000000   \n",
      "4  loove danz a fourthreenine threeticent fuliht ...               0.750000   \n",
      "\n",
      "   CER (character Error Rate)  Cosin ->  Blue ->  \\\n",
      "0                    0.200000       0.3      0.1   \n",
      "1                    0.042857       NaN      NaN   \n",
      "2                    0.000000       NaN      NaN   \n",
      "3                    0.000000       NaN      NaN   \n",
      "4                    0.285714       NaN      NaN   \n",
      "\n",
      "   Papers we used their dataset - measure of similarity  \\\n",
      "0                                                0.4      \n",
      "1                                                NaN      \n",
      "2                                                NaN      \n",
      "3                                                NaN      \n",
      "4                                                NaN      \n",
      "\n",
      "  Correct (CER threashold 0.2<=)  Accuracy  F1-Score  Precision  Recall  \\\n",
      "0                              1         1         0          0       0   \n",
      "1                              1         0         0          0       0   \n",
      "2                              1         1         1          1       1   \n",
      "3                              1         1         1          1       1   \n",
      "4                              0         0         0          0       0   \n",
      "\n",
      "   Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17  \\\n",
      "0          NaN         NaN     Model A     Model B   \n",
      "1          NaN  Dataset-1          NaN         NaN   \n",
      "2          NaN    Avg. WER         0.2        0.19   \n",
      "3          NaN    Avg. CER         0.1        0.09   \n",
      "4          NaN  Avg. Cosin        0.15         NaN   \n",
      "\n",
      "                                          input_text  \\\n",
      "0  correct: loflans are fourd three nine three-de...   \n",
      "1  correct: lufthansa four three nine three desce...   \n",
      "2  correct: lufthansa four three nine three desce...   \n",
      "3  correct: lufthansa four three nine three desce...   \n",
      "4  correct: loove danz a fourthreenine threeticen...   \n",
      "\n",
      "                                         target_text  \n",
      "0  lufthansa four three nine three descend to fli...  \n",
      "1  lufthansa four three nine three descend to fli...  \n",
      "2  lufthansa four three nine three descend to fli...  \n",
      "3  lufthansa four three nine three descend to fli...  \n",
      "4  lufthansa four three nine three descend to fli...  \n",
      "Training Data Columns: ['Dataset', 'Model', 'True Transcription', 'Predicted Transcription', 'WER (word Error rate)', 'CER (character Error Rate)', 'Cosin ->', 'Blue ->', 'Papers we used their dataset - measure of similarity', 'Correct (CER threashold 0.2<=)', 'Accuracy', 'F1-Score', 'Precision', 'Recall', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'input_text', 'target_text', '__index_level_0__']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/hmoradi/Downloads/colab_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████| 18876/18876 [00:05<00:00, 3630.61 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████| 4720/4720 [00:01<00:00, 3576.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch from DataLoader:\n",
      "input_ids: torch.Size([8, 64])\n",
      "attention_mask: torch.Size([8, 64])\n",
      "labels: torch.Size([8, 64])\n",
      "decoder_input_ids: torch.Size([8, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Epoch 0: 100%|████████████████████████████████████| 2360/2360 [03:52<00:00, 10.16it/s, loss=0.673]\n",
      "Epoch 1: 100%|████████████████████████████████████| 2360/2360 [03:53<00:00, 10.13it/s, loss=0.518]\n",
      "Epoch 2: 100%|████████████████████████████████████| 2360/2360 [03:52<00:00, 10.15it/s, loss=0.459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Loss: 0.5842615766608614\n",
      "Sample Predictions: ['hapag lloyd six five three good afternoon radar contact fly heading of two one zero call you back with climb  ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha', 'csa six one one ruzyne tower continue approach', 'cleared to land runway three one csa six bravo charlie    cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared', 'tower  one  tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower', 'csa seven five one runway one three clear to land wind zero six zero degrees six knots']\n",
      "Sample Labels: ['hapag lloyd six five three good afternoon radar contact fly heading of two one zero call you back with climb', 'csa six three one ruzyne tower continue approach', 'clear to land runway three one csa six bravo charlie', 'tower five one', 'csa seven five one runway one three clear to land wind zero six zero degrees six knots']\n",
      "WER: 2.0966628539321333, CER: 1.9397311914874855\n",
      "\n",
      "Sample Input, Prediction, and Label:\n",
      "Sample 1:\n",
      "Input: correct: hapag lloyd six five three good afternoon radar contact fly heading of two one zero call you back with climb\n",
      "Prediction: hapag lloyd six five three good afternoon radar contact fly heading of two one zero call you back with climb  ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha\n",
      "Label: hapag lloyd six five three good afternoon radar contact fly heading of two one zero call you back with climb\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "Input: correct: the bredancys ay sixty one rusing at our comtine approach\n",
      "Prediction: csa six one one ruzyne tower continue approach\n",
      "Label: csa six three one ruzyne tower continue approach\n",
      "--------------------------------------------------\n",
      "Sample 3:\n",
      "Input: correct: cleared to lind runway three one csa six bravo charlie\n",
      "Prediction: cleared to land runway three one csa six bravo charlie    cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared cleared\n",
      "Label: clear to land runway three one csa six bravo charlie\n",
      "--------------------------------------------------\n",
      "Sample 4:\n",
      "Input: correct: toweroor shuttle oeone five on\n",
      "Prediction: tower  one  tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower tower\n",
      "Label: tower five one\n",
      "--------------------------------------------------\n",
      "Sample 5:\n",
      "Input: correct: csa seven five one runway one three cleare to land wind zero six zero wegrees six knots\n",
      "Prediction: csa seven five one runway one three clear to land wind zero six zero degrees six knots\n",
      "Label: csa seven five one runway one three clear to land wind zero six zero degrees six knots\n",
      "--------------------------------------------------\n",
      "[2024-12-26 22:09:03,175] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Stop right there!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stop right there!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmoradi/Downloads/colab_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from evaluate import load\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF:\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "file_path = 'llm_evaluation_summary.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Initial Data Sample:\\n\", data.head())\n",
    "print(f\"Dataset size before cleaning: {data.shape}\")\n",
    "\n",
    "data = data.dropna(subset=[\"True Transcription\", \"Predicted Transcription\"]) # .head(500)\n",
    "data = data[data[\"True Transcription\"].str.strip() != \"\"]\n",
    "data = data[data[\"Predicted Transcription\"].str.strip() != \"\"]\n",
    "print(f\"Dataset size after cleaning: {data.shape}\")\n",
    "\n",
    "data[\"input_text\"] = data[\"Predicted Transcription\"].apply(lambda text: f\"correct: {text}\")\n",
    "data[\"target_text\"] = data[\"True Transcription\"]\n",
    "print(\"Processed Data Sample:\\n\", data.head())\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "hf_train_data = Dataset.from_pandas(train_data)\n",
    "hf_test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "print(\"Training Data Columns:\", hf_train_data.column_names)\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in seq]\n",
    "        for seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "hf_train_data = hf_train_data.map(tokenize_function, batched=True)\n",
    "hf_test_data = hf_test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "hf_train_data = hf_train_data.select_columns([\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "hf_test_data = hf_test_data.select_columns([\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"max_length\", max_length=64)\n",
    "train_dataloader = DataLoader(hf_train_data, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(hf_test_data, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "print(\"Sample batch from DataLoader:\")\n",
    "for batch in train_dataloader:\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    break\n",
    "\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = len(train_dataloader) * 3\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    labels = [[(token if token != -100 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"Sample Predictions:\", decoded_preds[:5])\n",
    "    print(\"Sample Labels:\", decoded_labels[:5])\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    cer = cer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "predictions = []\n",
    "labels_list = []\n",
    "input_texts = []  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        eval_loss += outputs.loss.item()\n",
    "        \n",
    "        \n",
    "        batch_predictions = torch.argmax(outputs.logits, dim=-1).tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "        labels_list.extend(batch[\"labels\"].tolist())\n",
    "        \n",
    "        \n",
    "        if len(input_texts) < 5:  \n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            input_texts.extend(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "eval_loss /= len(eval_dataloader)\n",
    "print(f\"Final Evaluation Loss: {eval_loss}\")\n",
    "\n",
    "\n",
    "metrics = compute_metrics((predictions, labels_list))\n",
    "print(f\"WER: {metrics['wer']}, CER: {metrics['cer']}\")\n",
    "\n",
    "\n",
    "decoded_predictions = tokenizer.batch_decode(predictions[:5], skip_special_tokens=True)\n",
    "decoded_labels = tokenizer.batch_decode(\n",
    "    [[(token if token != -100 else tokenizer.pad_token_id) for token in label] for label in labels_list[:5]],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nSample Input, Prediction, and Label:\")\n",
    "for i, (inp, pred, label) in enumerate(zip(input_texts[:5], decoded_predictions, decoded_labels)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Input: {inp}\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./transcription_correction_model\")\n",
    "tokenizer.save_pretrained(\"./transcription_correction_model\")\n",
    "raise SystemExit(\"Stop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (colab_env)",
   "language": "python",
   "name": "colab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
